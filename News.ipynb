{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import requests\n",
    "secret = '1c90d5b0adee41f3bbc237e4bf7c01d5'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://newsapi.org/v2/everything?'\n",
    "parameters = {\n",
    "    'q': 'Global Warming', # query phrase\n",
    "    'pageSize': 100,  # maximum is 100\n",
    "    'apiKey': secret # your own API key\n",
    "}\n",
    "response = requests.get(url, params=parameters)\n",
    "\n",
    "# Convert the response to JSON format and pretty print it\n",
    "response_json = response.json()\n",
    "pprint.pprint(response_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output  = response_json['articles']\n",
    "List_Url = []\n",
    "for i in range (0 , 6):\n",
    "    List_Url.append(output[i]['url'])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in response_json['articles']:\n",
    "    print(i['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "text_combined = ''\n",
    "# Loop through all the headlines and add them to 'text_combined' \n",
    "for i in response_json['articles']:\n",
    "    text_combined += i['title'] + ' ' # add a space after every headline, so the first and last words are not glued together\n",
    "# Print the first 300 characters to screen for inspection\n",
    "print(text_combined[0:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=40).generate(text_combined)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import tkinter as tk\n",
    "from newspaper import Article\n",
    "\n",
    "for i in range(0 , 1):\n",
    "\n",
    "    article = Article(List_Url[i])\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article.nlp()\n",
    "    print(f'Title:{article.title}')\n",
    "    print(f'Authors:{article.authors}')\n",
    "    print(f'Publication Date:{article.publish_date}')\n",
    "    print(f'Summary:{article.summary}')\n",
    "    sum= article.summary\n",
    "    analyse = TextBlob(article.text)\n",
    "    print(analyse.sentiment.polarity)\n",
    "    i+= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import tkinter as tk\n",
    "from newspaper import Article\n",
    "Text_List=[]\n",
    "for i in range(0 , 6):\n",
    "\n",
    "    article = Article(List_Url[i])\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article.nlp()\n",
    "    Text_List.append(article.text)\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "for i in range(0 , 6):\n",
    "    Text_List[i]\n",
    "    sentences = nltk.sent_tokenize(Text_List[i])\n",
    "    Words= nltk.word_tokenize(Text_List[i])\n",
    "    j=0\n",
    "    for sentence in sentences:\n",
    "        j+=1\n",
    "    print(\"Article sentences:\",i , j )\n",
    "    K=0\n",
    "    for word in Words:\n",
    "        K+=1\n",
    "    print(\"Article Words\", i , K)\n",
    "    i+=0 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = ''.join(Text_List)\n",
    "Text_Bow = str1.lower()\n",
    "print(Text_Bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import wordcloud\n",
    "import pandas as pd\n",
    "stop_wrods = set(stopwords.words(\"english\"))\n",
    "\n",
    "Words =nltk.word_tokenize(Text_List[5])\n",
    "filtered_words= []\n",
    "for w in Words:\n",
    "    if w not in stop_wrods:\n",
    "        filtered_words.append(w)\n",
    "\n",
    "for i in range(0 , len(filtered_words)):\n",
    "    filtered_words[i] = filtered_words[i].lower()\n",
    "\n",
    "frequency_dist = nltk.FreqDist(filtered_words)\n",
    "sorted(frequency_dist, key=frequency_dist.__getitem__, reverse= True)[0:30]\n",
    "large_words = dict([(k,v) for k , v in frequency_dist.items() if len(k) > 3 ] )\n",
    "\n",
    "frequency_dist = nltk.FreqDist(large_words)\n",
    "frequency_dist.plot(30,cumulative=False)\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=100,\n",
    "background_color=\"black\").generate_from_frequencies(frequency_dist)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "documents = Text_Bow.splitlines()\n",
    "count_vectorizer = CountVectorizer()\n",
    "bag_of_words =count_vectorizer.fit_transform(documents)\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "print(pd.DataFrame(bag_of_words.toarray(), columns=feature_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "values = tfidf_vectorizer.fit_transform(documents)\n",
    "feature_names =tfidf_vectorizer.get_feature_names()\n",
    "TFIDF_matrix = pd.DataFrame(values.toarray() , columns= feature_names)\n",
    "print(TFIDF_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "LDA =[]\n",
    "for i in range (0 , len(Text_List)):\n",
    "\n",
    "    x = re.sub(r'[^\\w]', ' ', Text_List[i])\n",
    "    x = re.sub(r'[\\d]', ' ', x )\n",
    "    x = re.sub(' +', ' ', x)\n",
    "    LDA.append(x)\n",
    "    i+=1\n",
    "df = pd.DataFrame(LDA) \n",
    "    \n",
    "# saving the dataframe \n",
    "df.to_csv('Articles.csv') \n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Text_List))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os.path\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "def load_data(path, file_name):\n",
    "    documents_list=[]\n",
    "    titles=[]\n",
    "    with open(os.path.join(path, file_name) , \"r\" , encoding = 'utf-8') as fin:\n",
    "        for line in fin.readlines():\n",
    "            text = line.strip()\n",
    "            documents_list.append(text)\n",
    "    print(\"Total Number of Dcouments: \" , len(documents_list))\n",
    "    titles.append(text[0:min(len(text) , 100)])\n",
    "    return documents_list, titles\n",
    "\n",
    "def preprocess_data(doc_set):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    p_stemmer = PorterStemmer()\n",
    "    Texts=[]\n",
    "    for i in doc_set:\n",
    "        raw=i.lower()\n",
    "        tokens= tokenizer.tokenize(raw)\n",
    "        stopped_tokens= [i for i in tokens if not i in en_stop]\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "        Texts.append(stemmed_tokens)\n",
    "    \n",
    "    return Texts\n",
    "\n",
    "def prepare_corpus(doc_clean):\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    doc_term_matrix= [ dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    return dictionary, doc_term_matrix\n",
    "\n",
    "\n",
    "def create_gensim_lsa_model(doc_clean , number_of_topics, words):\n",
    "    dictionary, doc_term_matrix = prepare_corpus(doc_clean)\n",
    "    lsamodel = LsiModel(doc_term_matrix , num_topics=number_of_topics , id2word= dictionary)\n",
    "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    return lsamodel\n",
    "\n",
    "    \n",
    "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, stop, step):\n",
    "        # generate LSA model\n",
    "        model = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "\n",
    "def plot_graph(doc_clean,start, stop, step):\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix,doc_clean,\n",
    "                                                            stop, start, step)\n",
    "    x = range(start, stop, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "start,stop,step=2,12,1\n",
    "plot_graph(clean_text ,start,stop,step)\n",
    "number_of_topics=3\n",
    "words= 10\n",
    "document_list, titles= load_data(\"/Users/ali/Desktop/Lessons/Web Social Media\", \"articles.csv\")\n",
    "clean_text = preprocess_data(document_list)\n",
    "model = create_gensim_lsa_model(clean_text, number_of_topics, words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import tkinter as tk\n",
    "from newspaper import Article\n",
    "for i in range(40 , 100):\n",
    "\n",
    "    article = Article(List_Url[i])\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article.nlp()\n",
    "    Publish_Date.append(article.publish_date)  \n",
    "    i+= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for i in Publish_Date:\n",
    "    res[i] = Publish_Date.count(i)\n",
    "    \n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for k, v in sorted(res.items(), key=lambda item: item[1])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_key = 1\n",
    "for key, value in res.items():\n",
    "      if value == desired_key:\n",
    "        del res[key]\n",
    "        break\n",
    "\n",
    "print(res)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {k: v for k, v in sorted(res.items(), reverse=True ,key=lambda item: item[1])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# create a dataset\n",
    "height = [9 , 6 , 6 , 5 , 4 , 3 ,3 ,3 , 3 ]\n",
    "bars = ('2022/4/4', '2022/4/21', '2022/4/13', '2022/4/29', '2022/4/22' , \"2022/4/7\" , \"2022/4/5\" , \"2022/4/19\" , \"2022/4/8\")\n",
    "x_pos = np.arange(len(bars))\n",
    "plt.title('Date Of Publish')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Articles')\n",
    "# Create bars with different colors\n",
    "plt.bar(x_pos, height, color = (0.5,0.1,0.5,0.6))\n",
    "\n",
    "# Create names on the x-axis\n",
    "plt.xticks(x_pos, bars , rotation=90)\n",
    "\n",
    "# Show graph\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25e1ce75cfb12f047913573b7bc7a98ece3d58460e313b6b0bb12faf016c0990"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
